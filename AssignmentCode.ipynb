{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemes de Recomanació"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest segon lliurament es programarà un **sistema de recomanació**, que posarà en correspondència un *usuari* amb *ítems* en funció de les seves preferències i interessos. \n",
    "En aquesta ocasió, implementareu un sistema de recomanació que assisteixi en una compra de supermercat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abans de començar\n",
    "\n",
    "\n",
    "**\\+ Durant la pràctica, solament es podran fer servir les següents llibreries**:\n",
    "\n",
    "`Pandas, Numpy, Itertools`\n",
    "\n",
    "*Nota: A més de les que ja es troben presents en la 1a cel·la i funcions natives de Python*\n",
    "\n",
    "**\\+ No es poden modificar les definicions de les funcions donades, ni canviar els noms de les variables i paràmetres ja donats**\n",
    "\n",
    "Això no implica però que els hàgiu de fer servir. És a dir, que la funció tingui un paràmetre anomenat `df` no implica que l'hàgiu de fer servir, si no ho trobeu convenient.\n",
    "\n",
    "**\\+ En les funcions, s'especifica que serà i de quin tipus cada un dels paràmetres, cal respectar-ho**\n",
    "\n",
    "Per exemple (ho posarà en el pydoc de la funció), `df` sempre serà indicatiu del `Pandas.DataFrame` de les dades. Durant els testos, els paràmetres (i específicament `df`) no contindran les mateixes dades que en aquest notebook, si bé si seran del mateix tipus! Per tant, no us refieu de què tinguin, per exemple, el mateix nombre de files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar les dades\n",
    "\n",
    "### **En aquesta cel·la no féu cap modificació**\n",
    "\n",
    "Descomprimeix els zips a la carpeta \"data\" automàticament. \n",
    "\n",
    "Descarregueu el zip amb les dades del campus i guardeu-lo dins de la carpeta del projecte. **No pugeu cap fitxer de dades a Github** (ja incloem al .gitignore un patró per ignorar-los)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pickle\n",
    "from os.path import join, dirname\n",
    "\n",
    "def unzip(file):\n",
    "    zip_ref = zipfile.ZipFile(file, 'r')\n",
    "    zip_ref.extractall('data')\n",
    "    zip_ref.close()\n",
    "    \n",
    "unzip('dades_p1.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les dades\n",
    "\n",
    "En aquest i futur notebooks farem servir dades reals corresponents a compres, concretament les utilitzades en el Kaggle Instacart Market Basket Analysis:\n",
    "https://www.kaggle.com/c/instacart-market-basket-analysis\n",
    "\n",
    "\n",
    "* **Order Products**: És el de major interès, conté la relació de productes comprats (`product_id`) per a cada conjunt de compra diferent (`order_id`). A aquests conjunts de compres ens hi referirem com a `ordres`, seguint la nomenclatura de les dades. A més, tot i que no ho farem servir, podríem arribar a saber en quin ordre s'han comprat els productes (`add_to_cart_order`) i inclús si ja s'havia comprat en alguna ordre anterior (`reordered`).\n",
    "\n",
    "* **Orders**: Aquest dataset ens permet relacionar una compra en concret (`order_id`) amb l'usuari que l'ha feta (`user_id`)\n",
    "\n",
    "* **Products**: Donat un `product_id` ens permet obtenir-ne més informació, com ara el nom (`product_name`), la secció en la qual es troba (`aisle_id`) o al departament al qual pertany (`department_id`). Aquests dos últims es complementen amb els conjunts **Aisles** i **Departments**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "### **En aquestes cel·les no feu cap modificació**\n",
    "\n",
    "Carrega les dades en un DataFrame Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df_order_prods = pd.read_csv(join('data', 'order_products__train.csv'))\n",
    "    df_orders = pd.read_csv(join('data', 'orders.csv'))[['order_id', 'user_id']]\n",
    "    df_prods = pd.read_csv(join('data', 'products.csv'))[['product_id', 'aisle_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementació\n",
    "\n",
    "Recordeu, seguiu els pydoc i compliu amb el que diuen!\n",
    "\n",
    "El primer que haurem de fer és construir una matriu que ens serveixi, d'alguna forma, com a indicatiu de preferències de cada persona. Per tal efecte, construirem una matriu $m\\times n$, de $m$ usuaris per $n$ items, on cada entrada $i,j$ serà el nombre de vegades que la persona $i$ a comprat l'item $j$.\n",
    "\n",
    "<img src=\"img/Mat.png\">\n",
    "\n",
    "Per saber de quin usuari és cada `order_id`, haureu de creuar el dataset `order_products` amb el `orders`. Una sola persona/usuari tindrà més d'una ordre, mireu quants cops ha comprat els mateixos productes.\n",
    "\n",
    "A més, les dades es componen de molts `product_id diferents`, hi ha massa diversitat entre usuaris. Per tant, per poder recomanar el que farem serà agregar les dades, en lloc de treballar per `product_id` ho farem per `aisle_id`, és a dir \"la secció\" del súper on es troba.\n",
    "\n",
    "Al llarg de la pràctica es parlarà de producte i/o item, perquè és la terminologia estàndard de recomanadors, però sempre serà en referència a `aisle_id` per aquesta pràctica!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_information(df_order_prods, df_orders, df_prods):\n",
    "    \"\"\"\n",
    "    Retorna el dataframe resultant de:\n",
    "        1. Creuar els datasets 'order_products' amb 'orders'.\n",
    "        2. Creuar el dataframe anterior amb 'products'.\n",
    "        Per creuar dos dataframes podeu utilitzar la funció pandas.DataFrame.merge\n",
    "\n",
    "    :param df_order_prods: DataFrame 'order_products'\n",
    "    :param df_orders: DataFrame 'orders'\n",
    "    :param df_prods: DataFrame 'products'\n",
    "    :return: DataFrame descrit prèviament   \n",
    "    \"\"\"\n",
    "    # Creuem df_order_prods i df_orders per saber les ordres de cada client\n",
    "    order_user = pd.merge(df_order_prods, df_orders, on='order_id')\n",
    "\n",
    "    # Creuem df_products amb el resultat anterior\n",
    "    order_user_prod = pd.merge(order_user, df_prods, on='product_id')\n",
    "    \n",
    "    # Retornem el DataFrame on tenim l'ID de cada client associat a l'ID del passadís del producte\n",
    "    return order_user_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_merged = merge_information(df_order_prods, df_orders, df_prods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_counts_table(df):\n",
    "    \"\"\"\n",
    "    Retorna un dataframe on les columnes són els `aisle_id`, les files `user_id` i els valors\n",
    "    el nombre de vegades que un usuari ha comprat un producte d'un `aisle_id`\n",
    "    \n",
    "    :param df: DataFrame original després de creuar-lo\n",
    "    :return: DataFrame descrit adalt\n",
    "    \"\"\"\n",
    "    # La funció groupby ens permet dividir les dades en grups segons certs criteris. Amb fill_value=0 canviem els NaN per 0.\n",
    "    return df.groupby(['user_id','aisle_id'], sort=False).size().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "def get_count(df, user_id, aisle_id):\n",
    "    \"\"\"\n",
    "    Retorna el nombre de vegades que un usuari ha comprat en un `aisle_id`\n",
    "    \n",
    "    :param df: DataFrame retornat per `build_counts_table`\n",
    "    :param user_id: ID de l'usuari\n",
    "    :param aisle_id: ID de la secció\n",
    "    :return: Enter amb el nombre de vegades que ha comprat\n",
    "    \"\"\"\n",
    "    return df.loc[user_id, aisle_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [5] of <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c227a01fcf33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdf_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_counts_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-e127151fafdd>\u001b[0m in \u001b[0;36mget_count\u001b[1;34m(df, user_id, aisle_id)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mEnter\u001b[0m \u001b[0mamb\u001b[0m \u001b[0mel\u001b[0m \u001b[0mnombre\u001b[0m \u001b[0mde\u001b[0m \u001b[0mvegades\u001b[0m \u001b[0mque\u001b[0m \u001b[0mha\u001b[0m \u001b[0mcomprat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maisle_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1492\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1015\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m                 \u001b[1;31m# This is an elided recursive call to iloc/loc/etc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'not applicable'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1912\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1913\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1801\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;31m# a scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_slice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m   2879\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'loc'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2881\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2883\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_invalid_indexer\u001b[1;34m(self, form, key)\u001b[0m\n\u001b[0;32m   3065\u001b[0m                         \"indexers [{key}] of {kind}\".format(\n\u001b[0;32m   3066\u001b[0m                             \u001b[0mform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3067\u001b[1;33m                             kind=type(key)))\n\u001b[0m\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3069\u001b[0m     \u001b[1;31m# --------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [5] of <class 'int'>"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_counts = build_counts_table(df_merged)\n",
    "    count = get_count(df_counts, 14, 5)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenim moltes dades en el nostre dataset, pel que és convenient que les reduïm una mica. Per començar a treballar recomanem que reduïu la mida a aproximadament 0.1 de l'original '(FRAC = 0.1)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    FRAC = 0.1\n",
    "    df_reduced = df_counts.sample(frac=FRAC, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesurament de similituds\n",
    "\n",
    "El primer pas per poder recomanar és definir una funció de similitud entre vectors. Siguin $x, y$ vectors, de les següents propostes implementa'n mínim una:\n",
    "\n",
    "* Distància euclidea (inversa): https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "\n",
    "$$sim(x, y) = \\frac{1}{1 + \\sqrt{\\sum_i(x_i-y_i)^2}}\\in [0, 1]$$\n",
    "\n",
    "* Similitud cosinus: https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "$$sim(x, y) = \\frac{x\\cdot y}{||x||\\hspace{0.1cm} ||y||} \\in [-1,1]$$\n",
    "\n",
    "* Correlació de Pearson: https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "\n",
    "$${\\displaystyle sim(x,y)={\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}} \\in [-1,1] \\\\ \\text{On }\\bar{x} = \\frac{1}{n} \\sum^n_i x_i\\text{ la mitja (i anàlogament per y)}$$\n",
    "\n",
    "Per implementar qualsevol d'aquestes únicament es permet l'ús de:\n",
    "\n",
    "* `np.sum`\n",
    "* `np.sqrt`\n",
    "* `np.power`\n",
    "* `np.dot`\n",
    "* `np.linalg.norm`\n",
    "* `np.mean`\n",
    "\n",
    "I s'ha de fer **sense bucles**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Tingueu en compte que les dues últimes funcions consideren valors negatius per exemples oposats (a diferència de la distància euclidea). En cas de fer servir alguna d'aquestes dues, pensa (més endavant) en com afectaran els negatius en la recomanació.\n",
    "\n",
    "En la similitud cosinus, vigileu amb casos on un usuari no ha comprat res, tindreu a ser una divisió entre 0.\n",
    "\n",
    "En la correlació de Pearson, haureu de considerar casos on algun dels dos exemples tingui variància 0, ja que aleshores estareu fent una divisió entre 0. En tals casos, podeu retornar un valor per defecte o alguna de les altres mesures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similarity(x, y):\n",
    "    \"\"\"\n",
    "    Definir quina de les similituds vols utilitzar  a l'execució.\n",
    "    \n",
    "    :param x: Primer vector\n",
    "    :param y: Segon vector\n",
    "    :return : Escalar (float) corresponent a la similitud\n",
    "    \"\"\"\n",
    "    return pearson(x, y)\n",
    "\n",
    "def euclid(x, y):\n",
    "    \"\"\"\n",
    "    Retorna la distància euclidiana inversa de dos vectors n-dimensionals.\n",
    "    \n",
    "    :param x: Primer vector\n",
    "    :param y: Segon vector\n",
    "    :return : Escalar (float) corresponent a la distància euclidiana\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.linalg.norm(x - y))\n",
    "\n",
    "def cosine(x, y):\n",
    "    \"\"\"\n",
    "    Retorna la similitud cosinus de dos vectors n-dimensionals.\n",
    "    \n",
    "    :param x: Primer vector\n",
    "    :param y: Segon vector\n",
    "    :return : Escalar (float) corresponent a la similitud cosinus\n",
    "    \"\"\"\n",
    "    if all(x == 0) or all(y == 0):\n",
    "        return 0\n",
    "    \n",
    "    return np.dot(x,y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "def pearson(x, y):\n",
    "    \"\"\"\n",
    "    Retorna la correlació de Pearson de dos vectors n-dimensionals.\n",
    "    \n",
    "    :param x: Primer vector\n",
    "    :param y: Segon vector\n",
    "    :return : Escalar (float) corresponent a la correlació de Pearson\n",
    "    \"\"\"\n",
    "    # Calculem la mitja tant de \"x\" com de \"y\".\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "    \n",
    "    # Obtenim el numerador a partir del sumatori del producte escalar de les mitjes càlculades prèviament.\n",
    "    num = np.sum(np.dot(x - mean_x, y - mean_y))\n",
    "    \n",
    "    # Mitjançant %timeit hem determinat que x**2 triga pràcticament el mateix que np.square(x) i és més ràpid que np.power(x,2) \n",
    "    # a l'hora de realitzar aquest càlcul.\n",
    "    var_x = np.sqrt(np.sum(x - mean_x)**2)\n",
    "    var_y = np.sqrt(np.sum(y - mean_y)**2)\n",
    "    \n",
    "    # Si el denominador és 0 llavors retornem 0 per evitar errors.\n",
    "    if var_x == 0.0 or var_y == 0.0:\n",
    "        return 0.\n",
    "    \n",
    "    return num / (var_x * var_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(similarity(np.asarray([1, 1, 1]), np.asarray([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriu de similituds\n",
    "\n",
    "Per fer recomanació col·laborativa existeixen dues opcions, fer un recomanador basat en usuaris o un en ítems:\n",
    "\n",
    "* Recomanador basat en usuaris:\n",
    "Considera la matriu $M\\times N: \\text{usuaris}\\times\\text{items}$, per recomanar t'hauràs de basar en les similituds entre els usuaris.\n",
    "\n",
    "* Recomanador basat en items:\n",
    "Considera la matriu $M\\times N: \\text{items}\\times\\text{usuaris}$, per recomanar t'hauràs de basar en les similituds entre els ítems.\n",
    "\n",
    "Construeix una matriu de mida $M\\times M$ on cada posició $i,j$ indica la distància entre l'element $i$ i el $j$. Així doncs, si estàs fent un recomanador basat en usuaris, `matriu[2, 3]` contindrà la similitud entre l'usuari 2 i el 3. En canvi, si l'estàs fent basat en ítems, `matriu[2, 3]` contindrà la similitud entre l'ítem 2 i el 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def generate_data(similarity_function, df_counts):\n",
    "    \"\"\"\n",
    "    Retorna una matriu de mida M x M on cada posició \n",
    "    indica la similitud entre usuaris (resp. ítems).\n",
    "    \n",
    "    :param similarity_function: Funció que calcularà la similitud \n",
    "        entre usuaris (resp. ítems)\n",
    "    :param df_counts: Dataframe que conté el nombre de vegades que \n",
    "        un usuari ha comprat en un `aisle_id`\n",
    "    :return simils: Matriu numpy de mida M x M amb les similituds.\n",
    "    \"\"\"\n",
    "    # Obtenim tots els usuaris.\n",
    "    user_buys = df_counts.values.tolist()\n",
    "    m = len(user_buys)\n",
    "    # Creem una matriu M x M inicialment buida segons la quantitat d'usuaris que tenim.\n",
    "    simils = np.empty((m, m))\n",
    "    \n",
    "    # Emprant tqdm podem visualitzar una barra de progrés per saber quant falta per acabar l'execució de l'algorisme. Bastant\n",
    "    # útil degut als temps de computació elevats de la pràctica.\n",
    "    # En cas d'error amb tqdm, només cal substituir la línia de sota per for i in range(m):\n",
    "    for i in tqdm_notebook(range(m)):\n",
    "        first_user = user_buys[i]\n",
    "        for j in range(len(user_buys)):\n",
    "            # Si l'índex es troba en la matriu superior realitzem el càlcul de similitud.\n",
    "            if i < j: \n",
    "                second_user = user_buys[j]\n",
    "                simil = similarity_function(np.asarray(first_user), np.asarray(second_user))\n",
    "                simils[i][j] = float(\"{0:.5f}\".format(simil))\n",
    "            \n",
    "            # Si ens trobem en la diagonal, assignem un 0 com a valor.\n",
    "            elif i == j: \n",
    "                simils[i][i] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(similarity_function, df_counts):\n",
    "    \"\"\"\n",
    "    Retorna una matriu de mida M x M on cada posició \n",
    "    indica la similitud entre usuaris (resp. ítems).\n",
    "    \n",
    "    :param similarity_function: Funció que calcularà la similitud \n",
    "        entre usuaris (resp. ítems)\n",
    "    :param df_counts: Dataframe que conté el nombre de vegades que \n",
    "        un usuari ha comprat en un `aisle_id`\n",
    "    :return : Matriu numpy de mida M x M amb les similituds.\n",
    "    \n",
    "    sklearn.metrics.pairwise_distances\n",
    "    \"\"\"         \n",
    "    # Per saber si treballarem amb les funcions implementades anteriorment o amb funció matricial, comprovarem \n",
    "    # si la funció és None (més ràpid per a la funció matricial).      \n",
    "    if similarity_function is None:\n",
    "        \n",
    "        # Matriu de similitud base (calculem tots els productes escalars)\n",
    "        base_matrix = df_counts.values\n",
    "        base_matrix_prod = np.dot(base_matrix, base_matrix.T)\n",
    "\n",
    "       # Magnitud quadrada dels vectors de preferència (nombre d'aparicions)\n",
    "        square_mag = np.diag(base_matrix_prod)\n",
    "\n",
    "        # Magnitud quadrada inversa\n",
    "        inv_square_mag = 1 / square_mag\n",
    "        inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
    "\n",
    "        # Inversa de la magnitud\n",
    "        inv_mag = np.sqrt(inv_square_mag)\n",
    "\n",
    "        # Semblança del cosinus (multiplicant elements per magnituds inverses)\n",
    "        cosine = base_matrix_prod * inv_mag\n",
    "        cosine = cosine.T * inv_mag\n",
    "        \n",
    "        # Omplim amb 0 els valors de les diagonals\n",
    "        np.fill_diagonal(cosine, 0)\n",
    "        \n",
    "        return cosine\n",
    "    \n",
    "    # En cas de no treballar amb matrius, realitzarem el càlcul amb la funció de la cel·la anterior.\n",
    "    else:             \n",
    "        # Obtenim tots els usuaris.\n",
    "        user_buys = df_counts.values.tolist()\n",
    "        m = len(user_buys)\n",
    "        # Creem una matriu M x M inicialment buida segons la quantitat d'usuaris que tenim.\n",
    "        simils = np.empty((m, m))\n",
    "\n",
    "        # Emprant tqdm podem visualitzar una barra de progrés per saber quant falta per acabar l'execució de l'algorisme. Bastant\n",
    "        # útil degut als temps de computació elevats de la pràctica.\n",
    "        # En cas d'error amb tqdm, només cal substituir la línia de sota per for i in range(m):\n",
    "        for i in tqdm_notebook(range(m)):\n",
    "            first_user = user_buys[i]\n",
    "            for j in range(len(user_buys)):\n",
    "                # Si l'índex es troba en la matriu superior realitzem el càlcul de similitud.\n",
    "                if i < j: \n",
    "                    second_user = user_buys[j]\n",
    "                    simil = similarity_function(np.asarray(first_user), np.asarray(second_user))\n",
    "                    simils[i][j] = float(\"{0:.5f}\".format(simil))\n",
    "\n",
    "                # Si ens trobem en la diagonal, assignem un 0 com a valor.\n",
    "                elif i == j: \n",
    "                    simils[i][i] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per cridar aquesta funció, el primer paràmetre pot ser:\n",
    "\n",
    "* Alguna de les funcions que has programat abans (*euclid*, *cosine* o *pearson*) (~@1h 30min treballant directament amb valors de numpy, ~@20h a partir de pandas pur)\n",
    "* Opcionalment (no és obligatori fer-ho) podeu programar una funció que treballi específicament amb matrius (i no vectors). Si ho feu, cal gestionar-ho quan es rep `None`. No totes les funcions anteriorment anomenades són fàcils (ni intuïtives, ni hi caben en memòria) d'aplicar en forma matricial.  (@5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        with open('similarities.pkl', 'rb') as fp:\n",
    "            similarities = pickle.load(fp)\n",
    "    except:\n",
    "        #similarities = similarity_matrix(similarity, df_reduced)\n",
    "        similarities = similarity_matrix(None, df_reduced)\n",
    "        with open('similarities.pkl', 'wb') as fp:\n",
    "            pickle.dump(similarities, fp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generació de prediccions\n",
    "\n",
    "Per fer recomanació col·laborativa, necessitem una funció que ens doni un valor de quant bona seria la recomanació. En el nostre cas i amb les nostres dades, volem una funció que ens indiqui quants cops compraria un usuari un producte donat.\n",
    "\n",
    "* Si esteu fent un recomanador basat en usuaris, la puntuació per a un usuari $u$ i ítem $j$ és\n",
    "\n",
    "$$pred(u, i) = \\hat{r}_{u,i} = \\frac{\\sum_{p\\neq u,r_{p,i}>0} sim(u, p)\\cdot r_{p,i}}{\\sum_{p\\neq u,r_{p,i}>0} sim(u, p)}$$\n",
    "\n",
    "On $r_{u,i}$ indica el nombre de vegades que l'usuari $u$ ha comprat l'l'ítem $i$.\n",
    "\n",
    "És a dir, per cada usuari $p$ diferent de $u$ si aquest usuari ha comprat algun cop el producte $i$, la similitud entre $p$ i $u$ multiplicada pel nombre de vegades que l'usuari $p$ ha comprat l'l'ítem $i$ ($r_{p,i}$).\n",
    "\n",
    "Pondera't per la suma de les similituds.\n",
    "\n",
    "* Anàlogament, si està basat en ítem, la puntuació per a un usuari $u$ i ítem $j$ és\n",
    "\n",
    "$$pred(u, i) = \\hat{r}_{u,i} = \\frac{\\sum_{j\\neq i,r_{u,j}>0} sim(i, j)\\cdot r_{u,j}}{\\sum_{j\\neq i,r_{u,j}>0} sim(i, j)}$$\n",
    "\n",
    "On $r_{u,i}$ indica el nombre de vegades que l'usuari $u$ ha comprat l'ítem $j$.\n",
    "\n",
    "És a dir, per cada ítem $j$ diferent de $i$ si l'usuari al qui recomanem ha comprat l'ítem $j$, la similitud entre $i$ i $j$ multiplicada pel nombre de vegades que l'usuari al qui recomanem $u$ ha comprat l'ítem $j$ ($r_{u,j}$)\n",
    "\n",
    "Pondera't per la suma de les similituds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixeu-vos que, sigui quin sigui el cas, al final estem fent el producte vectorial entre dos vectors. Concretament, el producte vectorial entre les similituds i les compres. Fes una funció que calculi aquest resultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(sims, counts, user_mean, means):\n",
    "    \"\"\"\n",
    "    * Si estàs implementant basat en usuaris:\n",
    "        Donades les similituds i el nombre de vegades que l'usuari ha comprat\n",
    "        cada ítem, retorna la predicció que indica quants cops compraria \n",
    "        l'usuari un nou ítem.\n",
    "    \n",
    "        :param sims: Similituds entre usuaris\n",
    "        :param counts: Nombre de vegades que l'usuari ha comprat cada ítem,\n",
    "        :return : Predicció (float) que indica quants cops compraria l'usuari un ítem.\n",
    "        \n",
    "        \n",
    "    * Si estàs implementant basat en items:\n",
    "        Donades les similituds i el nombre de vegades que l'item a recomanar ha\n",
    "        estat comprat per cada usuari, retorna la predicció que indica quants cops\n",
    "        compraria l'usuari un nou ítem.\n",
    "    \n",
    "        :param sims: Similituds entre items\n",
    "        :param counts: Nombre de vegades que l'ítem ha estat comprat per cada usuari\n",
    "        :return : Predicció (float) que indica quants cops compraria l'usuari un ítem.\n",
    "    \"\"\"\n",
    "    # Restem el nombre de productes comprats per cada usuari i la seva mitjana.\n",
    "    norms = np.subtract(counts, means)\n",
    "\n",
    "    # Calculem el numerador realitzant el producte escalar entre el vector de similituds i norms, tenint en compte únicament\n",
    "    # els casos en els que l'ítems ha sigut comprat (counts > 0) i descartant així els casos en els que la similitud és 0.\n",
    "    numerator = np.dot(sims[counts > 0], norms[counts > 0])\n",
    "\n",
    "    # Obtenim l'índex dels usuaris que han comprat l'ítem i calculem el denominador amb el sumatori del vector de similituds \n",
    "    # on els usuaris han comprat com a mínim una vegada.\n",
    "    buyers = np.where(counts != 0)[0]\n",
    "    denominator = sims[buyers].sum()\n",
    "    \n",
    "    return user_mean + (numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(user, item, df, similarities):\n",
    "    \"\"\"\n",
    "    Extreu les similituds i el nombre de vegades que un usuari ha comprat un ítem\n",
    "    (resp. nombre de vegades que cada usuari ha comprat l'ítem) i crida a la funció \n",
    "    anterior per calcular les prediccions.\n",
    "    \n",
    "    :param user: ID de l'usuari per la predicció\n",
    "    :param item: ID de l'ítem per la predicció\n",
    "    :param df: Dataframe que conté el nombre de vegades que un usuari \n",
    "        ha comprat en un `aisle_id`\n",
    "    :param similarities: Matriu de similituds\n",
    "    :return : Retorna un escalar (float) amb la predicció\n",
    "    \n",
    "    \"\"\"\n",
    "    # S'han realitzat algunes modificacions en la crida de calc_score per tal d'implementar la millora 1) descrita més avall.\n",
    "    \n",
    "    # Calculem la mitjana de compres (entre tots els passadissos) dels usuaris i obtenim la mitjana de l'usuari en qüestió.\n",
    "    # Podem obtenir la mitjana de l'usuari directament substituint les 2 línies per user_purchase_mean = df.loc[user].mean().\n",
    "    purchases_mean = df.mean(axis=1)\n",
    "    user_purchase_mean = purchases_mean[user]\n",
    "    \n",
    "    # Nombre de vegades que ha comprat l'ítem en qüestió cada ususari\n",
    "    users_item_purchases = np.asarray(df[item].values)\n",
    "    \n",
    "    # Vector de similituds de l'usuari en qüestió respecte la resta d'usuaris\n",
    "    user_index = df.index.get_loc(user)\n",
    "    users_similarities = similarities[user_index]\n",
    "    \n",
    "    # Cridem calc_score amb el vector de similituds, el nombre de vegades que cada usuari ha comprat l'ítem, la mitjana de\n",
    "    # compres de l'usuari en qüestió i la mitjana de compres de tots els usuaris.\n",
    "    return calc_score(users_similarities, users_item_purchases, user_purchase_mean, purchases_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5831981238428392\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(score(df_reduced.index[0], df_reduced.columns[0], df_reduced, similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feu una funció que donat un usuari calculi per cada item que no ha comprat la puntuació d'aquest. La funció retorna els $N$ items més ben puntuats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "def recommend_n_items(user_id, df, similarities, N):\n",
    "    \"\"\"\n",
    "    Donat un usuari calcula per cada ítem que no ha comprat la puntuació d'aquest. \n",
    "    La funció retorna els $N$ ítems més ben puntuats.\n",
    "    \n",
    "    :param user_id: Identificador de l'usuari\n",
    "    :parma df: Dataframe que conté el nombre de vegades que un usuari \n",
    "        ha comprat en un `aisle_id`\n",
    "    :param similarities: Matriu de similituds\n",
    "    :param N: Nombre d'ítems que volem que siguin recomanats.\n",
    "    :return : Llista amb els IDs dels ítems recomanats\n",
    "    \"\"\"\n",
    "    # Creem una cúa prioritària\n",
    "    pq = PriorityQueue() \n",
    "    \n",
    "    # Obtenim l'ubicació dels possibles ítems a recomanar (ítems no comprats per l'usuari).\n",
    "    no_comprats = df.loc[user_id] == 0 \n",
    "    posibles_recom = df.loc[user_id].loc[no_comprats].index.tolist()\n",
    "    for item in posibles_recom:\n",
    "        # Guardem l'ítem a la cúa prioritària un cop coneixem el seu \"score\".\n",
    "        valor = score(user_id, item, df, similarities)\n",
    "        pq.put((-valor, item))\n",
    "      \n",
    "    recommended_products, i = [], 0\n",
    "    \n",
    "    # Seleccionem els N ítems amb un millor valor per recomanar.\n",
    "    while not pq.empty() and i < N:\n",
    "        next_item = pq.get()\n",
    "        recommended_products.append(next_item[1])\n",
    "        i += 1\n",
    "        \n",
    "    return recommended_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92, 41, 123, 120, 3, 38, 37, 116, 28, 31]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(recommend_n_items(df_reduced.index[0], df_reduced, similarities, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possibles millores \n",
    "\n",
    "\n",
    "**0) Utilització completa de les dades:**\n",
    "\n",
    "Fer servir `df_original` tindrà (possiblement) resultats més fiables, però trigarà molt més que amb la versió reduida `df`. Pots canviar el `FRAC` a valors més alts ($\\leq 1$) per utilitzar més dades, però compte perque potser la matriu $M\\times M$ no hi cabrà en memòria.\n",
    "\n",
    "**1) Normalització: Prediccions escalades al domini de l'usuari:**\n",
    "Els usuaris compren en diferent mesura els productes, un més quantitat, altres menys. Fent servir la següent formula, escalem la predicció  a la mitja de l'usuari:\n",
    "$$pred(u, i) = \\hat{r}_{u,i} = \\bar{r_u} + \\frac{\\sum_{p\\neq u,r_{p,i}>0} sim(u, p)\\cdot (r_{p,i}-\\bar{r_b})}{\\sum_{p\\neq u,r_{p,i}>0} sim(u, p)}$$\n",
    "on $\\bar{r_u}$ és la mitjana de compres de l'usuari *u*.\n",
    "    \n",
    "**2) Valor del nombre d'elements codificats:**\n",
    "Redueix la similitud entre els usuaris si el nombre de productes és baix o descarta (en entrenament) aquells usuaris amb un petit nombre de productes comprats.\n",
    "\n",
    "**3) Augment de la similitud:**\n",
    "Incrementeu el pes als usuaris que són realment similars (~ = 1)\n",
    "\n",
    "**4) Selecció d'usuaris semblants:**\n",
    "Només s'utilitza un subconjunt d'usuaris similars, descartant tots aquells usuaris poc similars.\n",
    "\n",
    "\n",
    "Totes aquestes tècniques es poden aplicar d'igual manera en la recomanació col·laborativa per usuaris o ítems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "\n",
    "Per als usuaris que tens a continuació, quins productes els hi recomanaries (**fins a un màxim de 5**) que compressin segons el que ja han comprat?\n",
    "\n",
    "https://www.kaggle.com/t/f5b5030ef8cc4b5dbe985e6033878c75\n",
    "\n",
    "Si feu modificacions al codi original de cara a millorar els resultats pel Kaggle, feu una còpia d'aquest notebook i treballeu-hi allà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_test_products = pd.read_csv(join('data', 'order_products__test.csv'))\n",
    "    df_test_orders = pd.read_csv(join('data', 'orders__test.csv'))[['order_id', 'user_id']]\n",
    "    df_test_merged = merge_information(df_test_products, df_test_orders, df_prods)\n",
    "    \n",
    "    df_test_counts = build_counts_table(df_test_merged)\n",
    "    df_all = df_reduced.append(df_test_counts)\n",
    "    df_all = df_all.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tingueu en compte que si voleu fer un canvi a les similarities, heu d'esborrar l'arxiu similarities_test.pkl** (però haureu de recalcular, amb el temps que això suposi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try: \n",
    "        with open('similarities_test.pkl', 'rb') as fp:\n",
    "            similarities_test = pickle.load(fp)\n",
    "    except:\n",
    "        #similarities_test = similarity_matrix(similarity, df_all)\n",
    "        similarities_test = similarity_matrix(None, df_all)\n",
    "        with open('similarities_test.pkl', 'wb') as fp:\n",
    "            pickle.dump(similarities_test, fp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_submission = pd.DataFrame(columns=['user_id', 'products_list'])\n",
    "\n",
    "    for user_id in df_test_counts.index:\n",
    "        user_recos = recommend_n_items(user_id, df_all, similarities_test, 5)\n",
    "        df_submission = df_submission.append(\n",
    "            {\n",
    "                'user_id': user_id,\n",
    "                'products_list': ' '.join(map(str, user_recos))\n",
    "            }, \n",
    "            ignore_index=True)\n",
    "\n",
    "    df_submission.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id     products_list\n",
      "0  300097  113 132 90 10 82\n",
      "1  300130  113 90 132 10 82\n",
      "2  300111  113 82 10 90 118\n",
      "3  300007  113 82 90 10 118\n",
      "4  300107  113 82 132 90 10\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(df_submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quan tingueu l'arxiu submission.csv generat, podeu pujar-ho a Kaggle com una submission per veure el vostre score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
